[
  {
    "path": "posts/2021-05-24-popularity-of-first-names-over-time-with-gganimate/",
    "title": "Popularity of first names over time with gganimate",
    "description": {},
    "author": [
      {
        "name": "Daniel Banki",
        "url": "https://twitter.com/banki_daniel"
      }
    ],
    "date": "2020-07-31",
    "categories": [
      "gganimate"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData import & cleaning\r\nAnimation with gganimate\r\nConclusion\r\n\r\nIntroduction\r\nWhen I was researching my family tree some time ago, I was surprised to encounter the same couple of first names over and over again. My feeling was that this trend has changed over the years and that there is now more variability in first names. Getting data on the popularity of first names in, say, the 18th century was not very realistic. So I chose a somewhat less ambitious goal and collected data on Hungarian first names between 2004 and 2020. To show how first name popularity changed over this period, I created an animation using the R package gganimate. Although the period I looked at is quite short (and recent), it seems that the distribution of first names at the end of the period is indeed less unequal than at the beginning.\r\nData import & cleaning\r\nThe data I used is publicly available here and here—the authority responsible for publishing this information changed its website some years ago, hence the two sources. Data from each year is stored in a separate excel file; the structure of the files is very similar, but some data cleaning is required.\r\nFirst, we load the necessary libraries:\r\n\r\n\r\n# libraries # \r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(conflicted)\r\nlibrary(here)\r\nlibrary(readxl)\r\nlibrary(gganimate)\r\nlibrary(gapminder)\r\n\r\n\r\nconflict_prefer(\"View\", \"gganimate\")\r\nconflict_prefer(\"filter\", \"dplyr\")\r\n\r\n\r\n\r\nThen, we import the data files, combine them, and organise them into a tidy data format. For better readability, the code chunk that contains the data cleaning is hidden by default. \r\n\r\nShow data cleaning\r\n\r\n\r\n# data path - one excel file for each year\r\n\r\ndata_path <- paste0(here(), \"/first_names_data/\")\r\npaths <- dir(path = data_path, pattern = \"*.xls\")\r\n\r\n\r\n\r\n# read files into list\r\n\r\nall <- map(paste0(data_path, paths), read_excel)\r\nnames(all) <- parse_number(paths)\r\n\r\n\r\n\r\n# convert list into dataframe; add year as extra column\r\n\r\nall_df = bind_rows(all, .id = \"id\") %>% \r\n  discard(~all(is.na(.))) %>% \r\n  select(matches('id|utón')) %>% \r\n  select(-matches(\"Második\"))\r\n\r\n\r\n# name columns---needed for some later cleaning\r\n\r\nnames(all_df) <- c(\"year\", \"male_names1\", \"male_counts\",\r\n                   \"female_names1\", \"female_counts1\", \"female_counts2\",\r\n                   \"female_names2\", \"male_names2\")\r\n\r\n\r\n# required values need to be pulled from various columns\r\n\r\nall_df <- all_df %>% \r\n  mutate_at(c(\"female_counts1\", \"female_counts2\"), as.character)\r\n\r\nall_df <- all_df %>% \r\n  unite(\"male\",\r\n        c(\"male_names1\", \"male_names2\"),\r\n        na.rm = TRUE) %>% \r\n  unite(\"female\",\r\n        c(\"female_names1\", \"female_names2\"),\r\n        na.rm = TRUE) %>% \r\n  unite(\"female_counts\",\r\n        c(\"female_counts1\", \"female_counts2\"),\r\n        na.rm = TRUE)\r\n\r\n\r\n# organise into tidy data\r\n\r\nname_stock <- all_df %>%\r\n  pivot_longer(cols = \r\n    c(\"male\", \"female\"),\r\n    names_to = \"gender\",\r\n    values_to = \"name\") %>% \r\n  mutate(\r\n    count = ifelse(\r\n      gender == \"male\",\r\n      as.numeric(male_counts),\r\n      as.numeric(female_counts)), \r\n    year = as.numeric(year)) %>% \r\n  select(-female_counts, -male_counts)\r\n\r\nsave(name_stock, file = \"name_stock.RData\")\r\n\r\n\r\n\r\n The tidy output looks like this:\r\n\r\n\r\nhead(name_stock)\r\n\r\n\r\n# A tibble: 6 x 4\r\n   year gender name      count\r\n  <dbl> <chr>  <chr>     <dbl>\r\n1  2004 male   LÁSZLÓ   349027\r\n2  2004 female MÁRIA    442192\r\n3  2004 male   ISTVÁN   344481\r\n4  2004 female ERZSÉBET 342162\r\n5  2004 male   JÓZSEF   323510\r\n6  2004 female ILONA    230013\r\n\r\nFor example, in 2004 there were 442 192 women named Mária in Hungary—which is remarkable in a country of around 10 million people.\r\nAlthough the 100 most common names (both male and female) are available for each year, for the purposes of the animation I will focus only on the 25 most common female names.\r\n\r\n\r\nwomen <- name_stock %>% \r\n  filter(gender == \"female\") %>%\r\n  group_by(year) %>% \r\n  mutate(\r\n    rank = min_rank(-count) * 1, \r\n    count_1000 = count / 1000) %>% \r\n  slice(1:25) %>% \r\n  ungroup()\r\n\r\n\r\n\r\nAnimation with gganimate\r\nIn the next step, we create the animation with gganimate. Broadly speaking, we need three parts for the animation: the static part, the transition part, and the design part. In the static part, we determine what we need on each frame (or snapshot)—just like with a regular ggplot2 plot. For example, we want to flip the two axes, order names by their popularity, etc. In the transition part, we use gganimate to move from one snapshot to the next. In this case, we are interested in how first name popularity changed over the years, so we use the variable ‘year’ to transition between snapshots. The third part, design, is optional; its purpose is to make the animation look a bit nicer.\r\n\r\n\r\np <- ggplot(women, aes(rank, group = name)) + \r\n  # static part\r\n  geom_col(aes(y = count_1000)) +\r\n  geom_text(aes(y = 0, label = paste(name, \" \")),\r\n            vjust = 0.2,\r\n            hjust = 1, \r\n            size = 2) +\r\n  coord_flip(clip = \"off\", expand = FALSE) +\r\n  scale_x_reverse() +\r\n  # transition part with gganimate\r\n  transition_states(year, \r\n                    transition_length = 4,\r\n                    state_length = 1) +\r\n  ease_aes('cubic-in-out') + \r\n  labs(title = \"Popularity of Hungarian female names between 2004 and 2020\",\r\n       y = \"Count (000s)\",\r\n       x = \"\",\r\n       subtitle = 'Year: {closest_state}') +\r\n  # design part\r\n  theme_bw() + \r\n  theme(\r\n    axis.text.y = element_blank(),\r\n    axis.ticks.y = element_blank(),\r\n    plot.margin = unit(c(5.5, 5.5, 5.5, 60), \"points\"),\r\n    plot.title = element_text(\r\n      size = 14,\r\n      hjust = 0.5,\r\n      vjust = 0.5, \r\n      margin = margin(t = 20, b = 20)),\r\n    plot.subtitle = element_text(\r\n      size = 14,\r\n      hjust = 0.5,\r\n      vjust = 0.5, \r\n      margin = margin(b = 10)\r\n      )\r\n    )\r\n\r\n\r\n\r\n Let’s see the animation:\r\n\r\n\r\nanimate(p, nframes = 100)\r\n\r\n\r\n\r\n\r\nConclusion\r\nThe animation shows that the popularity of the most common first names decreased over time. This means that we are less likely to encounter a Mária or an Erzsébet than some years ago, while many, previously uncommon names are becoming more popular. This trend suggests that we are unlikely to observe a handful of extremely popular names in the future, because people seem to choose from an increasingly large set of names, which results in a less unequal distribution of names.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-24-popularity-of-first-names-over-time-with-gganimate/preview.png",
    "last_modified": "2021-05-25T01:31:08+02:00",
    "input_file": "popularity-of-first-names-over-time-with-gganimate.knit.md",
    "preview_width": 1021,
    "preview_height": 545
  },
  {
    "path": "posts/2021-05-24-learning-about-street-names-using-r-an-example-from-budapest/",
    "title": "Learning about street names using R: an example from Budapest",
    "description": {},
    "author": [
      {
        "name": "Daniel Banki",
        "url": "https://twitter.com/banki_daniel"
      }
    ],
    "date": "2020-02-09",
    "categories": [
      "Budapest",
      "tidyverse",
      "Wikipedia"
    ],
    "contents": "\r\n\r\nContents\r\nProject description\r\nStreet names in Budapest\r\nIdentifying the relevant streets\r\nRule 1: number of words\r\nRule 2: contains first name\r\n\r\nMatching names with professions\r\nWikidata search\r\nExtracting people’s professions\r\n\r\nAssigning profession labels\r\nLabel lookup table\r\nAssigning the final label\r\nVisualizing the results & conclusion\r\n\r\n\r\nProject description\r\nWhat can we learn about a city from its street names?\r\nWhen I was growing up in Budapest (Hungary), I certainly didn’t think much about this question. But as I started exploring the city and its history more consciously, I kept bumping into interesting and unusual street names.\r\nStreet names preserve valuable information about—among other things—a city’s geographical features, its history, as well as its famous inhabitants. Who are the people who have streets named after them, and what did they do to deserve such a merit?\r\nThis question is quite broad, and many local historians all over the world have surely spent decades working on exactly such topics. My goal with this project is a lot less ambitious: I focus only on a single city (Budapest) and on the professions of people who had streets named after them. Capturing a single aspect of people’s lives certainly wouldn’t satisfy a historian, but I hope this project can show that one can learn something meaningful about street names by using some readily available datasets and running a few lines of R code.\r\nOverview of the project\r\nIf we want to know the professions of people who had streets named after them, we can proceed in four steps. First, we get all the streets in Budapest. Second, we identify which streets were named after a person. Third, we look up each person’s profession(s). Finally, we assign a final label to each person (if the person had multiple professions).\r\nStreet names in Budapest\r\nThe Hungarian Postal Service (Hungary’s major postal service provider) maintains a database of all postcodes in the country. Almost all places in the country (including some larger cities) are identified by a single postcode. This isn’t the case in Budapest: postcodes there vary even within a district. Because postcodes are not unique, every street of the city must be listed in this database (with its corresponding postcode).\r\nBefore we import the data, let’s load the necessary packages:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(conflicted)\r\nlibrary(RCurl)\r\nlibrary(httr)\r\nlibrary(WikidataR)\r\nlibrary(rvest)\r\nlibrary(forcats)\r\n\r\nconflict_prefer(\"filter\", \"dplyr\")\r\n\r\n\r\n\r\nNext we import the data, remove some unnecessary columns, and remove duplicated street names. Note: there are duplicates in the database for two reasons. First, a street or its segments can have more than one postcode. Second, streets with the same name may exist in various districts.\r\n\r\n# import the relevant sheet from the excel file\r\nbp_streets <- read_excel(\"bp_streets.xlsx\", sheet = 3)\r\n\r\n# get rid of some unnecessary columns\r\nbp_streets <- bp_streets %>% select(IRSZ, UTCANÉV, UTÓTAG, KER)\r\n\r\n# rename columns\r\ncolnames(bp_streets) <- c(\"postcode\", \"street_name\", \"street_type\", \"district\")\r\n\r\n# remove duplicates\r\nbp_streets <- bp_streets %>% distinct(street_name, .keep_all = TRUE)\r\n\r\nThis leaves us with 6210 unique street names.\r\nIdentifying the relevant streets\r\nHow do we know which streets were named after people without checking the street names one by one? We rely on two simple rules that, applied simultaneously, allow us to identify most such streets. First, the street name must consist of at least two words. Second, it must contain a Hungarian first (or given) name. Although we capture a large portion of names using these two rules, we miss some names such as some foreigners’ names and one-word names that only have a last name.\r\nRule 1: number of words\r\nOne-word names are identified by a lack of whitespace; we discard them.\r\n\r\n\r\nbp_streets <- bp_streets %>%\r\n  mutate(has_ws = str_detect(bp_streets$street_name, \" \")) %>%\r\n  filter(has_ws == TRUE)\r\n\r\n\r\n\r\nIn the next step, we split street names into three columns: each word goes into a separate column. I chose three columns because it’s fairly common to have three-word street names in Budapest, but longer street names are relatively rare.\r\n\r\n\r\nbp_streets <- bp_streets %>%\r\n  separate(street_name, into = c(\"first\", \"second\", \"third\"), sep = \" \", remove = FALSE)\r\n\r\n\r\n\r\nThere are 1003 streets in Budapest that have at least two words.\r\nRule 2: contains first name\r\nIn Hungary there is a list of first names that people can legally have. We use this feature to identify the relevant streets. That is, if any of the words from the street name matches a legally recognized first name, we conclude that the street was indeed named after a person. Why are we considering all three words (if they exist) from the street’s name, and not just the second and the third? Both approaches would yield similar results, but it’s not uncommon to have streets with a given name in the first column with the following logic: for example, John [given name] general [occupation or other identifier] street.\r\nThe list of legally available names is managed by the Research Institute for Linguistics of the Hungarian Academy of Sciences, and it is available as a txt file from their website. (There is a separate list for male and female names.) We use the getURLContent function from the RCurl package to get the pages’ contents.\r\n\r\n\r\n# get male names\r\nmen_list <- RCurl::getURLContent(\"http://www.nytud.mta.hu/oszt/nyelvmuvelo/utonevek/osszesffi.txt\")\r\n\r\nmen_list <- men_list %>%\r\n  str_split(., \"\\n\") %>%\r\n  unlist() %>%\r\n  as.data.frame()\r\n\r\n# get string length, remove if not between 1 and 20\r\nmen_list <- men_list %>%\r\n  mutate(s_cont = str_count(.)) %>%\r\n  filter(s_cont %in% c(1:20))\r\n\r\n\r\n# get female names\r\nwomen_list <- getURLContent(\"http://www.nytud.mta.hu/oszt/nyelvmuvelo/utonevek/osszesnoi.txt\")\r\n\r\nwomen_list <- women_list %>%\r\n  str_split(., \"\\n\") %>%\r\n  unlist() %>%\r\n  as.data.frame()\r\n\r\nwomen_list <- women_list %>%\r\n  mutate(s_cont = str_count(.)) %>%\r\n  filter(s_cont %in% c(1:20))\r\n\r\n\r\n\r\nNow we combine the two lists:\r\n\r\n\r\ncombined_names <- rbind(men_list, women_list) %>% select(-s_cont)\r\n\r\ncolnames(combined_names) <- \"names\"\r\n\r\n\r\n\r\nWe check if any of the words from the street name matches a first name.\r\n\r\n\r\nbp_streets_names <- bp_streets %>%\r\n  filter_at(vars(first, second, third), any_vars(. %in% combined_names$names))\r\n\r\n\r\n\r\nBy filtering out streets with no matching first names, we eliminated 122 streets.\r\nAt the same time, we introduced two types of errors. First, we didn’t pick up certain relevant streets: mainly those named after foreigners and those with Hungarian given names that don’t exist today. Second, we classified certain streets as named after a person when, in reality, their names refer to something else. The second source of error is probably less consequential—we can at least partially correct it when we look up people’s professions.\r\nMatching names with professions\r\nWe identified 881 streets that were likely named after people. In this part, we look up each person’s profession. First, we search for the person’s name on Wikidata and we extract the person’s Wikidata ID. Based on this ID we then get information about the person, including his or her profession.\r\nWikidata search\r\nI considered two options to find people’s professions: scrape the data from individual Wikipedia pages or query Wikidata in a structured way. I opted for the second option—some people’s professions won’t be available this way, but the data we get are more consistent, and are also easier to handle.\r\nWikidata is a knowledge base that contains information in a structured format; Wikipedia relies on this information. For our purposes, two things are important: items and properties. The person we are interested in is called an item. It has a unique identifier and several properties. A property is, for example, the person’s sex, date of birth, or profession.\r\nWe use the WikidataR package for all Wikidata-related queries.\r\nFirst, let’s do a Wikidata search with each person’s name (using the find_item function). We set the limit to one result per search—otherwise many related findings would be included as well (e.g., the person’s spouse). We assume that the first result is the person of interest.\r\n\r\n\r\ndata <- purrr::map(bp_streets_names$street_name, find_item, limit = 1)\r\n\r\n\r\n\r\nNot all searches return a result. We remove those that don’t, then we extract the Wikidata IDs.\r\n\r\n\r\n# remove zero list elements\r\ndata <- Filter(length, data) %>% flatten()\r\n\r\n# look up IDs\r\nname_id <- purrr::map(data, \"id\") %>% unlist()\r\n\r\n\r\n\r\nAlthough many lesser-known people have Wikidata pages, some don’t. This most likely means that we undersample people who were relevant locally but not nationally. If these people’s professions differ significantly from those of nationally relevant people, then our final result can be misleading.\r\nWe have length(name_id) unique IDs. Let’s now get data for these people (using the get_item function).\r\n\r\n\r\nname_info <- purrr::map(name_id, get_item) %>% flatten()\r\n\r\n\r\n\r\nWe now have a list that contains a large amount of information about each person. In the next step, we find and extract the relevant properties.\r\nExtracting people’s professions\r\nWe define two functions to automate the extraction and formatting of properties.\r\n\r\n\r\n# get_elem: function to extract specific value\r\nget_elem <- function(i) {\r\n  out <- i[[\"mainsnak\"]][[\"datavalue\"]][[\"value\"]][[\"id\"]]\r\n\r\n  out\r\n}\r\n# if one row returns an error, return NA instead of throwing an error message\r\nget_elem_na <- possibly(get_elem, otherwise = NA)\r\n\r\n# extract_property: extract properties into a vector\r\nextract_property <- function(data, property) {\r\n\r\n  # extract information about property\r\n  out <- extract_claims(data, property) %>% flatten()\r\n\r\n  out <- map(out, get_elem_na)\r\n\r\n  # if more than one profession is listed, concatenate them\r\n  out <- map(out, paste, collapse = \",\") %>% unlist()\r\n\r\n  out\r\n}\r\n\r\n\r\n\r\nWe then extract the relevant properties: profession (P106), instance (P31), and ‘different from’ (P1889). Why do we need the last two properties?\r\nInstance determines if the item is a human being. As we searched on Wikidata and extracted the ID of the first search result in each case, it is possible that instead of finding the right person we found a non-human item. The property ‘different from’ denotes the ID of an item that is different from another item, with which it is often confused. In other words, if two people have the same name, then this property would be non-empty. As there are people with common family and given names in our data, there will be instances in which we won’t be able to uniquely identify the person.\r\n\r\n\r\njobs <- extract_property(name_info, \"P106\")\r\ninst <- extract_property(name_info, \"P31\")\r\nmultiple <- extract_property(name_info, \"P1889\")\r\n\r\n\r\n\r\nWe combine the vectors containing property values. Then we filter based on the three properties: we keep only those observations in which a profession is available, the item refers to a human, and there aren’t multiple people with the same name.\r\n\r\n\r\ndesc_df <- tibble(name_id, jobs, inst, multiple)\r\n\r\ndesc_df_valid <- desc_df %>%\r\n  filter(jobs != \"NA\" & inst == \"Q5\" & multiple == \"NA\") %>%\r\n  select(name_id, jobs)\r\n\r\n\r\n\r\nThere are 508 individuals whom we can use in our later analysis.\r\nAssigning profession labels\r\nFor every person we may have multiple professions; we only keep the first three. Going forward, I will refer to these professions as the first, second, and third profession, which simply reflects the order in which they appear in Wikidata.\r\n\r\n\r\n# separate occupations into 3 columns - only keep first 3, discard the rest\r\ndesc_df_valid <- desc_df_valid %>%\r\n  separate(jobs, sep = \",\", into = c(\"occ1\", \"occ2\", \"occ3\"), remove = FALSE)\r\n\r\n\r\n\r\nBecause of the way Wikidata is structured, we don’t have the labels (e.g., writer) for the professions yet, only their item IDs. We take some preparatory steps to get the vector of unique IDs.\r\n\r\n\r\n# get_label: extract labels for item\r\nget_label <- function(i) {\r\n  id <- i$id\r\n  label <- i[[\"labels\"]][[\"en\"]][[\"value\"]]\r\n  out <- c(id, label)\r\n  out\r\n}\r\n\r\nget_label_na <- purrr::possibly(get_label, otherwise = NA)\r\n\r\n# get vector of jobs\r\njobs_c <- str_split(jobs, \",\") %>% unlist()\r\n\r\n# remove NAs (NA is character here)\r\njobs_c <- jobs_c[!jobs_c %in% c(\"NA\")] %>% unique()\r\n\r\n\r\n\r\nLabel lookup table\r\nWe then look up the label for each profession ID:\r\n\r\n\r\nprof_list <- WikidataR::get_item(jobs_list)\r\n\r\n\r\n\r\nThe next step is to create a lookup table, with IDs in the first and labels in the second column. Then we add the labels to our data frame containing people’s IDs.\r\n\r\n\r\nlabelled_list <- purrr::map(prof_list, get_label_na)\r\n\r\nlabelled_df <- do.call(rbind, labelled_list)\r\n\r\ncolnames(labelled_df) <- c(\"id\", \"label\")\r\n\r\nlabelled_df <- as_tibble(labelled_df)\r\n\r\ndesc_df_valid <- left_join(desc_df_valid, labelled_df, by = c(\"occ1\" = \"id\")) %>%\r\n  left_join(., labelled_df, by = c(\"occ2\" = \"id\")) %>%\r\n  left_join(., labelled_df, by = c(\"occ3\" = \"id\"))\r\n\r\n\r\n\r\nAssigning the final label\r\nFor some people we have more than one profession label. How do we decide which one to use in our analysis?\r\nOur goal is to determine the most common professions of those who had streets named after them. Ideally, we would like to cluster professions into professional groups, such as cultural, political figures, scientists, etc. However, I couldn’t find a sensible mapping between individual professions and higher-level categories. Instead, we take advantage of the fact that some people have various listed professions. If we can select the most common profession from the person’s professions, we have gone some way toward a smaller number of final categories (and with more people in each). For example, one of the people in our list had the following professions: university teacher, engineer, geodesist. The most common category based on the first professions is engineer; this person would then be classified as an engineer. Intuitively, a person is likely to have related labels; instead of assigning a rare profession (geodesist) to a higher-category (scientist), we assign it to a common element of that category (engineer).\r\n\r\n\r\n# sort professions by frequency; largest first\r\nrank_df <- as.data.frame(sort(table(desc_df_valid$label.x), decreasing = TRUE))\r\n\r\ncolnames(rank_df) <- c(\"label\", \"freq\")\r\n\r\n# convert three columns into a list\r\nocc_list <- desc_df_valid %>%\r\n  select(label, label.x, label.y) %>%\r\n  asplit(., 1)\r\n\r\n# get_min_rank: get most common label from each row\r\nget_min_rank <- function(x) {\r\n  x <- as.character(rank_df[min(match(x, rank_df$label, nomatch = length(rank_df$label) + 1)), 1])\r\n  x\r\n}\r\n\r\ndesc_df_valid$label_final <- purrr::map(occ_list, get_min_rank) %>% unlist()\r\n\r\n\r\n\r\nTo check if considering potentially three labels for each individual did make a difference, we calculate the number of times either the second or the third label was assigned as a final label.\r\n\r\n\r\nlength(which(desc_df_valid$label.x != desc_df_valid$label_final))\r\n\r\n\r\n[1] 102\r\n\r\nAlso, we check if (and how many) profession labels we managed to eliminate compared to collecting only a single label for every person.\r\n\r\n\r\nnlevels(as.factor(desc_df_valid$label.x)) - nlevels(as.factor(desc_df_valid$label_final))\r\n\r\n\r\n[1] 20\r\n\r\nIt seems that the final labels are indeed more homogeneous than the first profession labels.\r\nNow that we have the frequency of professions, let’s visualize this information. First, we prepare the data and lump infrequent categories into the category “Other”. A category is infrequent if there are fewer than 5 people with that profession.\r\n\r\n\r\nprof_freq <- as.data.frame(sort(table(desc_df_valid$label_final), decreasing = TRUE))\r\n\r\ncolnames(prof_freq) <- c(\"label\", \"freq\")\r\n\r\n# merge observations that have fewer than 5 entries into category \"Other\"\r\nprof_freq_cats <- as.data.frame(sort(table(fct_lump_min(desc_df_valid$label_final, min = 5)), decreasing = TRUE))\r\n\r\ncolnames(prof_freq_cats) <- c(\"label\", \"freq\")\r\n\r\n\r\n\r\nVisualizing the results & conclusion\r\nWe use ggplot to display the data visually:\r\n\r\n\r\nggplot(prof_freq_cats, aes(label, freq)) +\r\n  geom_bar(stat = \"identity\") +\r\n  geom_text(aes(label = freq), vjust = -1) +\r\n  labs(title = \"Most frequent professions\", x = \"\", y = \"Count\") +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +\r\n  ylim(0, max(prof_freq_cats$freq) + 50)\r\n\r\n\r\n\r\n\r\nThe results are in line with what can be expected: political figures (politician, soldier, military officer, military personnel), cultural figures (writer, poet, painter, sculptor), scientists (physicist, engineer, mathematician, historian) are the ones who have had streets named after them in Budapest. It would also be interesting to look at the era in which these people lived, their connections with Budapest, or if the distribution of professions was different at some point (e.g., following regime changes, which usually meant renaming some streets).\r\nAlthough the results look reasonable, it’s worth highlighting some aspects of this process once again that could introduce some (or even significant) bias. First, we may have left out a large number of people whose given names are not common, not Hungarian, or are formatted differently than common names. Second, we couldn’t find information about all people, and where we did find information, it may have been about another person. Third, we may have chosen the wrong label for some people: for example, the person is famous for being a poet, yet we categorized her as a politician, because politician (the more common label) was also one of her labels.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-24-learning-about-street-names-using-r-an-example-from-budapest/learning-about-street-names-using-r-an-example-from-budapest_files/figure-html5/unnamed-chunk-26-1.png",
    "last_modified": "2021-05-25T00:34:31+02:00",
    "input_file": "learning-about-street-names-using-r-an-example-from-budapest.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-24-estimating-ambulance-waiting-times-with-isochrones/",
    "title": "Estimating ambulance waiting times with isochrones",
    "description": {},
    "author": [
      {
        "name": "Daniel Banki",
        "url": "https://twitter.com/banki_daniel"
      }
    ],
    "date": "2020-01-19",
    "categories": [
      "leaflet",
      "maps",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nProject description\r\nGetting started\r\nIdentifying underserved areas\r\nAddresses of ambulance stations\r\nGeocoding\r\nIsochrones (drive-time maps)\r\n\r\nPopulation data\r\nResults and some thoughts\r\nLocation and availability of ambulances\r\nIsochrone and timing assumptions\r\nGEOSTAT data\r\n\r\nConclusion\r\nUseful resources\r\n\r\nProject description\r\nEmergency medical services always work against the clock. Though arrival time requirements for ambulances vary across countries, the objective in all cases is for help to arrive as soon as possible. But to know how to improve arrival times, we need to understand the realities of the current infrastructure. For example, if ambulance stations are located far away from certain areas, adding extra personnel or resources won’t improve health outcomes.\r\nUsing the existing infrastructure, I estimate what percentage of the Hungarian population lives more than 15 minutes away from an ambulance station. This estimate is not an accurate representation of people’s access to medical emergency services—I think about it rather as a starting point that can generate some valuable insights.\r\nA couple of notes before we proceed:\r\nI’m not an expert in working with geographic data. Fortunately, some people are. They are also kind enough to share their knowledge: I provide links to useful resources here.\r\nData: to fully reproduce this project, you will need two datasets and an API key from the location platform HERE. I provide details at the relevant steps.\r\nGetting started\r\nWhat percentage of Hungarians live more than 15 minutes away from an ambulance station?\r\nWe need two pieces of information to answer this question:\r\nThe areas that cannot be reached by an ambulance from an ambulance station within 15 minutes—I will refer to these areas as underserved areas.\r\nThe number of people who live in these underserved areas.\r\nFirst of all, let’s load the necessary packages:\r\n\r\n\r\nlibrary(here)\r\nlibrary(conflicted)\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\n\r\n# packages related to handling geographic data, mapping, etc.\r\nlibrary(rgdal)\r\nlibrary(sf)\r\nlibrary(hereR)\r\nlibrary(sp)\r\nlibrary(raster)\r\nlibrary(leaflet)\r\nlibrary(FRK)\r\nlibrary(rgeos)\r\nlibrary(widgetframe)\r\n\r\n\r\n# which package to prefer when a function is available in two packages\r\nconflict_prefer(\"select\", \"dplyr\")\r\nconflict_prefer(\"filter\", \"dplyr\")\r\nconflict_prefer(\"extract\", \"raster\")\r\n\r\n\r\n\r\nIdentifying underserved areas\r\nIf we want to identify underserved areas, first we need to know where ambulance stations are located—first physically, then using geographic coordinates. Then we can draw drive-time maps (so called isochrone maps) around each ambulance station. The areas that are not covered by any of these maps are the underserved areas.\r\nAddresses of ambulance stations\r\nI collected the physical addresses of ambulance stations from the Országos Mentőszolgálat (Hungarian Ambulance Service, the state provider of emergency medical care). As of 2019, Hungary had 255 ambulance stations. Since the official map on the Országos Mentőszolgálat’s website contained fewer than 255 stations, I added some ambulance stations manually. The output of this process was an excel file (available on my Github page), which I used as the basis for geocoding. (My list actually contains one extra station—I couldn’t identify which station was duplicated or is no longer operational.)\r\n\r\n\r\n# import manually cleaned addresses\r\naddresses <- read_excel(\"amb_stations.xlsx\")\r\n\r\n\r\n\r\nGeocoding\r\nOnce we have the physical addresses of ambulance stations, we need to convert them into geographic coordinates. This is a process called geocoding. To do this I used an API from the HERE platform. I chose this option because you can make a large number of requests for free, and there is an R package called hereR that provides an interface to it. If you plan to use their services, you need to create an account with them and generate an authentication key (create an account here). We use the geocode function from the hereR package to get the geographic coordinates. Though many alternatives exist, using the same package for geocoding and in later steps requires fewer changes in the data format.\r\n\r\n\r\n# secret API key from my .Rprofile; otherwise you don't need the line below\r\nkey <- getOption(\"key\")\r\n\r\n# replace \"key\" by your generated API key\r\nset_key(key)\r\n\r\ngc <- geocode(addresses$address)\r\n\r\n\r\n\r\nLet’s now plot the all the ambulance stations in the country using the leaflet function (from the leaflet package). As an optional step, I created a custom marker icon. (The icon was made by Freepik from www.flaticon.com.)\r\n\r\n\r\n# create a new marker icon for mapping --- optional step\r\nambulanceIcon <- makeIcon(\r\n  iconUrl = \"ambulance.png\",\r\n  iconWidth = 15, iconHeight = 15\r\n)\r\n\r\n# use manually set map bounds from EPSG:4237\r\nleaflet() %>%\r\n  fitBounds(lng1 = 16.11, lat1 = 45.74, lng2 = 22.9, lat2 = 48.58) %>%\r\n  addProviderTiles(\"CartoDB.Positron\", group = \"Greyscale\") %>%\r\n  addMarkers(data = gc, icon = ambulanceIcon)\r\n\r\n\r\npreserve0476fa582e3c5928\r\n\r\nAt first glance, the map looks reasonable—all the points seem to be located in the country, and their spatial distribution is also feasible.\r\nIsochrones (drive-time maps)\r\nNow that we know the location of each ambulance station in the country, it’s time we calculated their corresponding drive-time maps, using the isoline function from the hereR package. We specify that we require 15-minute (900-second) isochrones using the fastest route, where we travel by car, and traffic is disabled. What we get is a list of polygons, which may or may not intersect. Going forward, I will refer to this list of polygons as the isochrones polygon.\r\n\r\n\r\n# use geocodes to get isochrones\r\n\r\npoly <- isoline(gc,\r\n  range_type = \"time\", mode = \"car\",\r\n  type = \"fastest\", range = 900\r\n)\r\n\r\n\r\n\r\nLet’s visualize the isochrones polygon:\r\n\r\n\r\nleaflet() %>%\r\n  fitBounds(lng1 = 16.11, lat1 = 45.74, lng2 = 22.9, lat2 = 48.58) %>%\r\n  addProviderTiles(\"CartoDB.Positron\", group = \"Greyscale\") %>%\r\n  addPolygons(data = poly, weight = 1) %>%\r\n  addMarkers(data = gc, icon = ambulanceIcon)\r\n\r\n\r\npreserve537889ebc8f2f31f\r\n\r\nThough the area covered by the isochrones looks rather small, this should not be surprising: one can only drive on roads, which don’t take up a large area. At the same time, the population is distributed unequally—most people live close to roads, so many areas outside the isochrones are uninhabited.\r\nOnce we have these isochrones, we have two choices. We can either get the number of people living within these isochrones, and subtract this number from the country’s population. Alternatively, we can get the underserved areas first, and then calculate the number of people who live in those areas. The two approaches should yield very similar results; here I identify the underserved areas first, and work with the population data only after that.\r\nTo get the underserved areas, we do three things. First, we get geographic data for Hungary (i.e., a polygon, the shape of which is determined by Hungary’s border line). Second, we make sure that the two polygons (isochrones and Hungary) have the same coordinate reference system (CRS). Third, we subtract the isochrones polygon from Hungary’s polygon.\r\n\r\n\r\n# get the polygon for Hungary\r\nhu <- getData(\"GADM\", country = \"Hungary\", level = 0)\r\n\r\n\r\n\r\n\r\n\r\n# check that we indeed get Hungary's map\r\nleaflet() %>%\r\n  fitBounds(lng1 = 16.11, lat1 = 45.74, lng2 = 22.9, lat2 = 48.58) %>%\r\n  addProviderTiles(\"CartoDB.Positron\", group = \"Greyscale\") %>%\r\n  addPolygons(data = hu, color = \"#8CBA51\")\r\n\r\n\r\npreservebd12e51c15a83285\r\n\r\n\r\n\r\n# set CRS for the isochrones polygon --- its CRS should be the same as that of Hungary\r\npoly_sp <- as_Spatial(poly)\r\n\r\nproj4string(poly_sp) <- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\r\n\r\n\r\n\r\n\r\n\r\n# get the underserved areas polygon:\r\n# the function first takes the union of all the individual isochrones,\r\n# then it subtracts this new polygon from Hungary's polygon.\r\ndiff <- gDifference(hu, poly_sp)\r\n\r\n\r\n\r\nLet’s plot the underserved areas in red:\r\n\r\n\r\nleaflet() %>%\r\n  fitBounds(lng1 = 16.11, lat1 = 45.74, lng2 = 22.9, lat2 = 48.58) %>%\r\n  addProviderTiles(\"CartoDB.Positron\", group = \"Greyscale\") %>%\r\n  addPolygons(data = diff, color = \"#FF5D6C\", weight = 1)\r\n\r\n\r\npreserve9c3ce49408493eb9\r\n\r\nNow that we have these areas, it’s time to get Hungary’s population data.\r\nPopulation data\r\nThe population data comes from the GEOSTAT 2011 project, which is a European initiative to collect population data for each 1 km2 area in Europe (Data source population grid information: Eurostat, EFGS). The dataset can be downloaded from here.\r\nIntuitively, one can think of these data as cells in a large table, where each cell has a numerical value (i.e. population) attached to it. Note that this representation of the data differs from how the data were represented in the first half of this project. A further difference is that this dataset doesn’t use the same coordinate reference system. Intuitively, if we fail to correct for this latter difference, then a point with the same longitude and latitude values may describe a different physical location on two maps.\r\n\r\n\r\n# read in the downloaded dataset\r\npop_data <- read_csv(\"geostat_2011.csv\")\r\n\r\n# population data for Hungary\r\npop_data <- pop_data %>%\r\n  filter(CNTR_CODE == \"HU\") %>%\r\n  select(TOT_P, GRD_ID)\r\n\r\n\r\n# prepare the dataset so it can be later transformed into a more suitable format\r\npop_data <- pop_data %>% mutate(\r\n  lat = as.numeric(gsub(\".*N([0-9]+)[EW].*\", \"\\\\1\", GRD_ID)),\r\n  lon = as.numeric(gsub(\".*[EW]([0-9]+)\", \"\\\\1\", GRD_ID)) * ifelse(gsub(\".*([EW]).*\", \"\\\\1\", GRD_ID) == \"W\", -1, 1)\r\n)\r\n\r\n\r\npop_data$lat <- pop_data$lat * 1000\r\n\r\npop_data$lon <- pop_data$lon * 1000\r\n\r\npop_data_to_r <- pop_data %>%\r\n  select(lon, lat, TOT_P) %>%\r\n  rename(x = lon, y = lat, z = TOT_P)\r\n\r\n# put the data into a raster object: a table-like structure to store geographic data\r\ndf_raster <- rasterFromXYZ(pop_data_to_r, res = 1000, crs = \"+init=epsg:3035\")\r\n\r\ndiff_proj <- diff\r\n\r\n# transform the underserved areas polygon to EPSG:3035; the polygon and the raster object must have the same CRS\r\ndiff_proj <- spTransform(diff_proj, \"+init=epsg:3035\")\r\n\r\n\r\n\r\nAfter having prepared the data, we use the extract function from the raster package to get the number of people who are more than 15 minutes away from an ambulance station.\r\n\r\n\r\nuncovered_pop <- extract(df_raster, diff_proj)\r\n\r\nsum(uncovered_pop[[1]], na.rm = TRUE)\r\n\r\n\r\n[1] 2672746\r\n\r\nResults and some thoughts\r\nWe find that approximately 2.7 million people (or 27% of the population) live more than 15 minutes away from an ambulance station. But how realistic is this result?\r\nIt is difficult to say, but two pieces of information from Hungary might give us an idea. For example, a similar project in 2014 estimated that ambulances cannot reach 14% of the population within 15 minutes. The spokesperson for the Országos Mentőszolgálat (Hungarian Ambulance Service) claimed in an interview that emergency medical help arrived within 15 minutes in 77-78% of all cases (the source is available in Hungarian here). What matters ultimately is the time it takes for emergency medical help to arrive. Using isochrones to answer this important question is a good starting point, but we have made many implicit assumptions along the way. Let’s make some of these assumptions explicit and speculate a bit about what happens if a particular assumption is violated.\r\nLocation and availability of ambulances\r\nIf an ambulance is available, it can be either at the station or somewhere else (e.g., returning from a task). If an ambulance is not at its station, then its effective arrival time will differ from its predicted arrival time. Furthermore, ambulances need to be available at the time of the emergency call: capacity constraints can be a significant problem, and I suspect they often are. The situation is probably worse in sparsely populated areas, which often only have one ambulance station nearby.\r\nIsochrone and timing assumptions\r\nIn this project, we used isochrones that didn’t take traffic conditions into account. In reality, ambulances can arrive more slowly (e.g., in Budapest, during rush hours) or more quickly (e.g., in rural areas, where ambulances can be driven faster than the speed limit). We further assumed that the duration of both the emergency call and the ambulance’s leaving the station is zero.\r\nThe size of the affected population is quite sensitive to changes in the time we use for the isochrones. For example, rerunning the analysis with 20-minute isochrones yielded an underserved population of approximately 1.2 million people. Some assumptions probably make us underestimate the time needed for an ambulance to arrive: that ambulances can leave without delay; that there is no traffic in a traffic-heavy area; or that ambulances are available. Conversely, imposing other assumptions such as adhering to speed limits can mean that ambulances actually arrive sooner than predicted by our isochrones.\r\nGEOSTAT data\r\nThe GEOSTAT data give us a snapshot of Hungarian’s registered addresses in 2011. It is possible that some people migrated since then to other areas in the country, or even left the country. Furthermore, being registered at an address doesn’t mean that the individual actually lives there or spends most of her time there. For example, a person from a smaller town may be registered in her home town in an underserved area, but she works, attends university, or spends her leisure time in a place with better emergency access. Taken together, I suspect that at the time of an emergency people are actually closer to emergency services than the GEOSTAT population data suggest.\r\nConclusion\r\nUsing isochrone maps to estimate the arrival times of ambulances is a good starting point. It would be useful, however, to know more about how robust these estimates are to changes in the various assumptions underlying this approach.\r\nUseful resources\r\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow\r\nRSpatial\r\nCoordinate Reference Systems in R\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-24-estimating-ambulance-waiting-times-with-isochrones/preview.png",
    "last_modified": "2021-05-25T00:42:47+02:00",
    "input_file": "estimating-ambulance-waiting-times-with-isochrones.knit.md",
    "preview_width": 899,
    "preview_height": 497
  }
]
